{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pKEMtvrcEyYA"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets\n",
    "inspired from \\cite{saadany2020fake}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake news\n",
    "1. Satirical fake news from Al-Hudood and Al-Ahram Al-Mexici : Saadany et al. dataset\n",
    "https://github.com/sadanyh/Arabic-Satirical-Fake-News-Dataset/blob/master/fake_news.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtncBP_LEskm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3185 fake articles from Al-Hudood and Al-Ahram Al-Mexici.\n",
      "<bound method NDFrame.head of                                                    text label\n",
      "0     الأسد يختبئ داخل بيوت النازحين ليصطادهم عند عو...  fake\n",
      "1     مشج ع رياضي ينشغل بمتابعة المباراة عن ترديد ال...  fake\n",
      "2     كاريكاتير فارس قره بيت   السلطة تدعو لفك الارت...  fake\n",
      "3     الحكومة الأمريكية تحيل جواسيسها في المنطقة الع...  fake\n",
      "4     ترشيح لجائزة أكثر خبر مضل   كلب  يتحول  إلى دب...  fake\n",
      "...                                                 ...   ...\n",
      "3180  ترشيح لجائزة الخبر الأكثر إنحيازا    إسرائيل ت...  fake\n",
      "3181  ترشيح لجائزة الخاصة   فتيات يأكلن ويشربن في ال...  fake\n",
      "3182  العبادي يحذ ر المواطنين من الاستمرار بالتظاهر ...  fake\n",
      "3183  سائق باص يغادر المحطة رغم وجود فسحة تسع لراكب ...  fake\n",
      "3184  قال الصحفي الشهير مصطفى بكرى أنه لولا وجود الس...  fake\n",
      "\n",
      "[3185 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Define the path where the fake news text files are stored\n",
    "folder_path = './fake_news'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "fake_news_data = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):  # Only process .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Open the text file and read its contents\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            fake_news_data.append(content)\n",
    "\n",
    "# Create a DataFrame from the fake news text data\n",
    "satirical_fake_news = pd.DataFrame(fake_news_data, columns=['text'])\n",
    "satirical_fake_news['label'] = 'fake'  # Add the label\n",
    "\n",
    "\n",
    "# Shuffle the dataset\n",
    "satirical_fake_news = satirical_fake_news.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save the compiled dataset to a new CSV file\n",
    "satirical_fake_news.to_csv(\"fake_news_dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "num_fake_news=len(satirical_fake_news)\n",
    "print(f\"Sampled {num_fake_news} fake articles from Al-Hudood and Al-Ahram Al-Mexici.\")\n",
    "print(satirical_fake_news.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real news\n",
    "2. True news from BBC and CNN\n",
    "\n",
    "- method in https://arxiv.org/pdf/2011.00452 \n",
    "\n",
    "\"The real news dataset comprises 3185 articles from the BBC-News Arabic and CNN-Arabic news datasets which are available on the Sourceforge part of the Arabic Computational Linguistics project (https://sourceforge.net/projects/ar-text-mining/files/Arabic-Corpora/). \n",
    "\n",
    "To ensure that the fake and real datasets talk about similar themes, we chose the articles that belong to the politics section in the two real news corpora.\"\n",
    "\n",
    "\n",
    "- Data processing done manually:\n",
    "\n",
    "Only took \"middle east\" and \"world\" news folders from BBC and CNN news folders and put them together as the real_news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3185 real articles from BBC and CNN from SourceForge.\n",
      "<bound method NDFrame.head of                                                    text label\n",
      "0                                   ‭BBC Arabic‬ - ‮...  real\n",
      "1                                   ‭BBC Arabic‬ - ‮...  real\n",
      "2               CNNArabic.com - صحف: \"طوابير خامسة\" ...  real\n",
      "3                                ‭BBC Arabic‬ - ‮الش...  real\n",
      "4                                ‭BBC Arabic‬ - ‮الش...  real\n",
      "...                                                 ...   ...\n",
      "3180            CNNArabic.com - أمير قطر يجدد الدعوة...  real\n",
      "3181            CNNArabic.com - مدير CIA السابق ينتق...  real\n",
      "3182            CNNArabic.com - \"حماس\": شقيق سامي أب...  real\n",
      "3183            CNNArabic.com - موسيقيون يوقعون عريض...  real\n",
      "3184                                ‭BBC Arabic‬ - ‮...  real\n",
      "\n",
      "[3185 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Define the path where the fake news text files are stored\n",
    "folder_path = './real_news'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "real_news_data = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):  # Only process .txt files\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Open the text file and read its contents\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            real_news_data.append(content)\n",
    "\n",
    "\n",
    "# Create a DataFrame from the fake news text data\n",
    "true_news_sample = pd.DataFrame(real_news_data, columns=['text'])\n",
    "true_news_sample['label'] = 'real'  # Add the label\n",
    "\n",
    "\n",
    "# Shuffle the dataset\n",
    "true_news_sample = true_news_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Reduce the number of real news articles to match the number of fake news articles\n",
    "if len(true_news_sample) > num_fake_news:\n",
    "    true_news_sample = true_news_sample.head(num_fake_news)\n",
    "\n",
    "\n",
    "# Save the compiled dataset to a new CSV file\n",
    "true_news_sample.to_csv(\"real_news_dataset.csv\", index=False)\n",
    "\n",
    "\n",
    "# Display the shape and first few rows of the compiled dataset\n",
    "num_true_news=len(true_news_sample)\n",
    "print(f\"Sampled {num_true_news} real articles from BBC and CNN from SourceForge.\")\n",
    "print(true_news_sample.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of real data\n",
    "real dataset seems messy but fake dataset looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 3185 real articles (CLEANED) from BBC and CNN from SourceForge.\n",
      "<bound method NDFrame.head of                                                    text label\n",
      "0      الشرق الأوسط الأراضي الفلسطينية في العقد الأو...  real\n",
      "1      الشرق الأوسط البرلمان العراقي يطالب باستجواب ...  real\n",
      "2     . صحف طوابير خامسة بالكويت.. ونكتة تغضب السودا...  real\n",
      "3      الشرق الأوسط الاسد يدعو فرنسا للعب دور أكبر ف...  real\n",
      "4      الشرق الأوسط بالصور الطريق الى مواقع التمرد ب...  real\n",
      "...                                                 ...   ...\n",
      "3180  . أمير قطر يجدد الدعوة لعقد قمة عربية طارئة أم...  real\n",
      "3181  . مدير السابق ينتقد كشف أوباما لمذكرات عن تعذي...  real\n",
      "3182  . حماس شقيق سامي أبو زهري عذب حتى الموت بمصر ا...  real\n",
      "3183  . موسيقيون يوقعون عريضة لكشف استخدام أعمالهم ب...  real\n",
      "3184   العالم زعيم المعارضة في اليابان فوزنا يمثل ثو...  real\n",
      "\n",
      "[3185 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the real news dataset\n",
    "# Step 1: Load the Data\n",
    "true_news_sample_cleaned = pd.read_csv('real_news_dataset.csv')\n",
    "\n",
    "# Step 2: Clean the Text\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Keep only Arabic letters, spaces, and some punctuation\n",
    "    text = re.sub(r'[^أ-ي\\s،؛؟.]+', '', text) \n",
    "\n",
    "    # Normalize spaces (replace multiple spaces with a single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'text' column\n",
    "true_news_sample_cleaned['text'] = true_news_sample_cleaned['text'].apply(clean_text)\n",
    "\n",
    "# Step 3: Save the Cleaned Data\n",
    "true_news_sample_cleaned.to_csv('cleaned_real_news.csv', index=False)\n",
    "\n",
    "# Display the shape and first few rows of the compiled dataset\n",
    "num_real_news_df = len(true_news_sample_cleaned)\n",
    "print(f\"Sampled {num_real_news_df} real articles (CLEANED) from BBC and CNN from SourceForge.\")\n",
    "print(true_news_sample_cleaned.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets (real and fake)\n",
    "3. Tweets from (Alyoubi et al., 2023): https://www.mdpi.com/2076-3417/13/14/8209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = pd.read_csv('path_to_tweets.csv')  # Load the tweets dataset\n",
    "# tweets['label'] = tweets['label'].map({0: 'fake', 1: 'real'})  # Assuming 0 is fake and 1 is real in the tweet dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all datasets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (6370, 2)\n",
      "<bound method NDFrame.head of                                                    text label\n",
      "0      الشرق الأوسط الترقب يسود في الخليل بعد مواجها...  real\n",
      "1     فتاة تقبل بشقة واحدة وخاتم بألماسة وحيدة تقدير...  fake\n",
      "2        محافظ القليوبية  آلاف جنيه لكل مدينة وقرية ...  fake\n",
      "3     السيسي يؤك د أن ه يعتقل منافسيه على الرئاسة لي...  fake\n",
      "4      الشرق الأوسط العراق مقتل واصابة شخصا في حوادث...  real\n",
      "...                                                 ...   ...\n",
      "6365   الشرق الأوسط العفو الدولية تحذر من نقل معسكر ...  real\n",
      "6366   الشرق الأوسط الحكم بالإعدام على هنديا في الإم...  real\n",
      "6367  . محكمة سعودية تصدر أحكاما بحق عضوا في تنظيم ا...  real\n",
      "6368   الشرق الأوسط وفد إسرائيلي ينقب عن رفات جنود إ...  real\n",
      "6369  دليل   كيف تستغني عن المنطق في حياتك اليومية  ...  fake\n",
      "\n",
      "[6370 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets into one\n",
    "# final_dataset = pd.concat([satirical_fake_news, true_news_sample, tweets], ignore_index=True)\n",
    "final_dataset = pd.concat([satirical_fake_news, true_news_sample_cleaned], ignore_index=True)\n",
    "\n",
    "# Shuffle the final dataset\n",
    "final_dataset = final_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the shape and first few rows of the compiled dataset\n",
    "print(f\"Final dataset shape: {final_dataset.shape}\")\n",
    "print(final_dataset.head)\n",
    "\n",
    "# Save the compiled dataset to a new CSV file\n",
    "final_dataset.to_csv('compiled_real_fake_news_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
