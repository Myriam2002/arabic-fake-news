{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKEMtvrcEyYA"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# OLD\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import svm\n",
    "# import random\n",
    "\n",
    "\n",
    "#  NEW\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split for TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5096, 5000)\n",
      "(1274, 5000)\n",
      "(5096,)\n",
      "(1274,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../final_datasets/articles_dataset.csv')\n",
    "\n",
    "# Encode the labels (real/fake)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['label'])  # Assuming 'label' column has 'real' and 'fake' values\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the training data only, then transform both sets\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=50) #old\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # new: Increased features, added n-grams\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train_texts)\n",
    "X_test = tfidf_vectorizer.transform(X_test_texts)\n",
    "\n",
    "# Check the shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "- **Code source:** Support Vector Machine (SVM): https://www.kaggle.com/code/mehmetlaudatekman/text-classification-svm-explained\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning technique commonly applied to classification problems, like fake news detection. In this context, SVM works by separating real and fake news articles using a decision boundary based on the features extracted from Arabic text data. For instance, these features might include word frequencies, linguistic patterns, or even word embeddings tailored for Arabic, which capture contextual relationships in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Train the SVM classifier\n",
    "\n",
    "# OLD\n",
    "# svm_classifier = svm.SVC(kernel='linear', random_state=42) \n",
    "\n",
    "# # Perform cross-validation to confirm that the number of tfidf features was enough by checking consistency of accuracy among folds\n",
    "# cv_scores = cross_val_score(svm_classifier, X_train, y_train, cv=5) \n",
    "\n",
    "# # Print cross-validation results\n",
    "# print(f'Cross-Validation Scores: {cv_scores}')\n",
    "# print(f'Mean Accuracy: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}')\n",
    "\n",
    "\n",
    "# NEW\n",
    "svm_model = SVC(random_state=42) \n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],           # Regularization strength\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernels to try\n",
    "    'gamma': ['scale', 'auto'],       # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "    'degree': [2, 3, 4]               # Degree for polynomial kernel\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "# svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# NEW\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set with the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"SVM Results:\")\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST\n",
    "- **Code source:** eXtreme Gradient Boosting (XGBoost): https://www.kaggle.com/code/iamarjunchandra/text-classification-with-rnn-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:32:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy: 0.9757\n",
      "Precision: 0.9924\n",
      "Recall: 0.9618\n",
      "F1 Score: 0.9769\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Initialize the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# OLD\n",
    "# # Step 2: Train the classifier\n",
    "# xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Predict on the test set\n",
    "# y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# NEW\n",
    "# Step 2: Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],      # Step size shrinkage\n",
    "    'max_depth': [3, 5, 7],                 # Maximum depth of a tree\n",
    "    'n_estimators': [50, 100, 200],         # Number of boosted trees to fit\n",
    "    'subsample': [0.8, 1.0],                # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.8, 1.0],         # Fraction of features used per tree\n",
    "    'gamma': [0, 1, 5],                     # Minimum loss reduction required for a split\n",
    "    'reg_alpha': [0, 0.1, 1],               # L1 regularization term\n",
    "    'reg_lambda': [1, 10, 100]              # L2 regularization term\n",
    "}\n",
    "\n",
    "# Step 3: Perform Grid Search\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search_xgb.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search_xgb.best_score_)\n",
    "\n",
    "# Step 5: Evaluate on the test set with the best model\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Step 5: Print the evaluation results for XGBoost\n",
    "print(\"XGBoost Results:\")\n",
    "print(f'Accuracy: {accuracy_xgb:.4f}')\n",
    "print(f'Precision: {precision_xgb:.4f}')\n",
    "print(f'Recall: {recall_xgb:.4f}')\n",
    "print(f'F1 Score: {f1_xgb:.4f}')\n",
    "\n",
    "# Comparison with SVM"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
