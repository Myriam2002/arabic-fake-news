{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using CNN for Fake news detection","metadata":{}},{"cell_type":"markdown","source":"## Libraries needed","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:38:31.231680Z","iopub.execute_input":"2024-12-01T11:38:31.232091Z","iopub.status.idle":"2024-12-01T11:38:31.243221Z","shell.execute_reply.started":"2024-12-01T11:38:31.232053Z","shell.execute_reply":"2024-12-01T11:38:31.241883Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/all-datasets/tweets_dataset.csv\n/kaggle/input/all-datasets/combined_dataset.csv\n/kaggle/input/all-datasets/articles_dataset.csv\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport re # We use regular expressions for data cleaning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (accuracy_score,f1_score, roc_auc_score, roc_curve, # evaluatin metrics\n                             confusion_matrix,classification_report)\nimport tensorflow as tf\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\n\nplt.style.use('seaborn')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:38:31.250549Z","iopub.execute_input":"2024-12-01T11:38:31.250980Z","iopub.status.idle":"2024-12-01T11:38:31.260373Z","shell.execute_reply.started":"2024-12-01T11:38:31.250939Z","shell.execute_reply":"2024-12-01T11:38:31.259201Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\nfrom datasets import load_dataset\nfrom camel_tools.tokenizers.word import simple_word_tokenize\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport re\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:38:31.268715Z","iopub.execute_input":"2024-12-01T11:38:31.269618Z","iopub.status.idle":"2024-12-01T11:38:31.277543Z","shell.execute_reply.started":"2024-12-01T11:38:31.269566Z","shell.execute_reply":"2024-12-01T11:38:31.276310Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Dataset preprocessing and model implementation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport re\nfrom collections import Counter\n\n# Preprocessing function\ndef preprocess_text(text):\n    \"\"\"Clean and tokenize text.\"\"\"\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    return text.split()  # Tokenize by splitting on spaces\n\n# Vocabulary building\ndef build_vocabulary(texts, max_vocab_size=50000):\n    \"\"\"Build vocabulary from training texts only.\"\"\"\n    word_counts = Counter()\n    for text in texts:\n        word_counts.update(preprocess_text(text))\n    vocab = {'<pad>': 0, '<unk>': 1}\n    for word, _ in word_counts.most_common(max_vocab_size - 2):\n        vocab[word] = len(vocab)\n    return vocab\n\n# Dataset class\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, vocab, max_length=200):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        tokens = preprocess_text(self.texts[idx])\n        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n        if len(indices) < self.max_length:\n            indices = indices + [self.vocab['<pad>']] * (self.max_length - len(indices))\n        else:\n            indices = indices[:self.max_length]\n        return torch.tensor(indices), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# Define the TextCNN model\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n        self.convs = nn.ModuleList([\n            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n            for fs in filter_sizes\n        ])\n        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n        embedded = self.embedding(text).unsqueeze(1)  # (batch_size, 1, max_length, embedding_dim)\n        conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n        pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n        cat = self.dropout(torch.cat(pooled, dim=1))\n        return torch.sigmoid(self.fc(cat))\n\n# Training function\ndef train_model(model, train_loader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    all_preds, all_labels = [], []\n\n    for texts, labels in train_loader:\n        texts, labels = texts.to(device), labels.to(device)\n        optimizer.zero_grad()\n        predictions = model(texts).squeeze(1)\n        loss = criterion(predictions, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        all_preds.extend(predictions.detach().cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy, precision, recall, f1 = compute_metrics(all_labels, all_preds)\n    return epoch_loss / len(train_loader), accuracy, precision, recall, f1\n\n# Evaluation function\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for texts, labels in test_loader:\n            texts, labels = texts.to(device), labels.to(device)\n            predictions = model(texts).squeeze(1)\n            loss = criterion(predictions, labels)\n            epoch_loss += loss.item()\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy, precision, recall, f1 = compute_metrics(all_labels, all_preds)\n    return epoch_loss / len(test_loader), accuracy, precision, recall, f1\n\n# Compute evaluation metrics\ndef compute_metrics(y_true, y_pred):\n    y_pred_binary = (np.array(y_pred) >= 0.5).astype(int)\n    accuracy = accuracy_score(y_true, y_pred_binary)\n    precision = precision_score(y_true, y_pred_binary)\n    recall = recall_score(y_true, y_pred_binary)\n    f1 = f1_score(y_true, y_pred_binary)\n    return accuracy, precision, recall, f1\n\n# Experiment runner\ndef run_experiment(train_texts, train_labels, test_texts, test_labels, max_length=200, epochs=10):\n    vocab = build_vocabulary(train_texts)\n    train_dataset = TextDataset(train_texts, train_labels, vocab, max_length=max_length)\n    test_dataset = TextDataset(test_texts, test_labels, vocab, max_length=max_length)\n\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=64)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = TextCNN(\n        vocab_size=len(vocab),\n        embedding_dim=100,\n        n_filters=100,\n        filter_sizes=[3, 4, 5],\n        output_dim=1,\n        dropout=0.5,\n        pad_idx=vocab['<pad>']\n    ).to(device)\n\n    optimizer = optim.Adam(model.parameters())\n    criterion = nn.BCELoss()\n\n    for epoch in range(epochs):\n        train_loss, train_acc, train_prec, train_rec, train_f1 = train_model(model, train_loader, optimizer, criterion, device)\n        print(f'Epoch {epoch + 1} | Train Loss: {train_loss:.3f} | Acc: {train_acc:.3f} | Prec: {train_prec:.3f} | Rec: {train_rec:.3f} | F1: {train_f1:.3f}')\n\n    test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, test_loader, criterion, device)\n    print(f'\\nTest Results | Loss: {test_loss:.3f} | Acc: {test_acc:.3f} | Prec: {test_prec:.3f} | Rec: {test_rec:.3f} | F1: {test_f1:.3f}')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:38:31.279773Z","iopub.execute_input":"2024-12-01T11:38:31.280577Z","iopub.status.idle":"2024-12-01T11:38:31.310507Z","shell.execute_reply.started":"2024-12-01T11:38:31.280538Z","shell.execute_reply":"2024-12-01T11:38:31.309216Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Model calling and model running","metadata":{}},{"cell_type":"code","source":"# Load datasets\ndf_articles = pd.read_csv('/kaggle/input/all-datasets/articles_dataset.csv')\ndf_tweets = pd.read_csv('/kaggle/input/all-datasets/tweets_dataset.csv')\ndf_combined = pd.read_csv('/kaggle/input/all-datasets/combined_dataset.csv')\n\narticles_texts = df_articles['text'].tolist()\narticles_labels = df_articles['label'].map({'real': 1, 'fake': 0}).tolist()\n\ntweets_texts = df_tweets['text'].tolist()\ntweets_labels = df_tweets['label'].map({True: 1, False: 0}).tolist()\n\ncombined_texts = df_combined['text'].tolist()\ncombined_labels = df_combined['label'].map({'real': 1, 'fake': 0}).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:40:06.507467Z","iopub.execute_input":"2024-12-01T11:40:06.507918Z","iopub.status.idle":"2024-12-01T11:40:07.281789Z","shell.execute_reply.started":"2024-12-01T11:40:06.507865Z","shell.execute_reply":"2024-12-01T11:40:07.280145Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Run experiments\nprint(\"Experiment 1: Train on Tweets, Test on Articles\")\nrun_experiment(tweets_texts, tweets_labels, articles_texts, articles_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:40:11.182234Z","iopub.execute_input":"2024-12-01T11:40:11.182651Z","iopub.status.idle":"2024-12-01T11:42:35.928628Z","shell.execute_reply.started":"2024-12-01T11:40:11.182614Z","shell.execute_reply":"2024-12-01T11:42:35.927236Z"}},"outputs":[{"name":"stdout","text":"Experiment 1: Train on Tweets, Test on Articles\nEpoch 1 | Train Loss: 0.629 | Acc: 0.631 | Prec: 0.633 | Rec: 0.628 | F1: 0.630\nEpoch 2 | Train Loss: 0.404 | Acc: 0.832 | Prec: 0.820 | Rec: 0.852 | F1: 0.835\nEpoch 3 | Train Loss: 0.278 | Acc: 0.906 | Prec: 0.912 | Rec: 0.901 | F1: 0.906\nEpoch 4 | Train Loss: 0.196 | Acc: 0.941 | Prec: 0.942 | Rec: 0.939 | F1: 0.941\nEpoch 5 | Train Loss: 0.132 | Acc: 0.963 | Prec: 0.964 | Rec: 0.962 | F1: 0.963\nEpoch 6 | Train Loss: 0.099 | Acc: 0.973 | Prec: 0.974 | Rec: 0.972 | F1: 0.973\nEpoch 7 | Train Loss: 0.067 | Acc: 0.987 | Prec: 0.986 | Rec: 0.988 | F1: 0.987\nEpoch 8 | Train Loss: 0.054 | Acc: 0.988 | Prec: 0.988 | Rec: 0.988 | F1: 0.988\nEpoch 9 | Train Loss: 0.041 | Acc: 0.993 | Prec: 0.993 | Rec: 0.994 | F1: 0.993\nEpoch 10 | Train Loss: 0.039 | Acc: 0.989 | Prec: 0.991 | Rec: 0.987 | F1: 0.989\n\nTest Results | Loss: 1.308 | Acc: 0.665 | Prec: 0.666 | Rec: 0.996 | F1: 0.798\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(\"\\nExperiment 2: Train on Articles, Test on Tweets\")\nrun_experiment(articles_texts, articles_labels, tweets_texts, tweets_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:42:35.930614Z","iopub.execute_input":"2024-12-01T11:42:35.930999Z","iopub.status.idle":"2024-12-01T11:54:43.697626Z","shell.execute_reply.started":"2024-12-01T11:42:35.930963Z","shell.execute_reply":"2024-12-01T11:54:43.696408Z"}},"outputs":[{"name":"stdout","text":"\nExperiment 2: Train on Articles, Test on Tweets\nEpoch 1 | Train Loss: 0.367 | Acc: 0.826 | Prec: 0.853 | Rec: 0.893 | F1: 0.872\nEpoch 2 | Train Loss: 0.174 | Acc: 0.930 | Prec: 0.948 | Rec: 0.947 | F1: 0.948\nEpoch 3 | Train Loss: 0.082 | Acc: 0.971 | Prec: 0.978 | Rec: 0.978 | F1: 0.978\nEpoch 4 | Train Loss: 0.041 | Acc: 0.985 | Prec: 0.989 | Rec: 0.988 | F1: 0.989\nEpoch 5 | Train Loss: 0.022 | Acc: 0.994 | Prec: 0.996 | Rec: 0.995 | F1: 0.995\nEpoch 6 | Train Loss: 0.016 | Acc: 0.995 | Prec: 0.996 | Rec: 0.996 | F1: 0.996\nEpoch 7 | Train Loss: 0.010 | Acc: 0.997 | Prec: 0.998 | Rec: 0.998 | F1: 0.998\nEpoch 8 | Train Loss: 0.007 | Acc: 0.998 | Prec: 0.998 | Rec: 0.998 | F1: 0.998\nEpoch 9 | Train Loss: 0.004 | Acc: 0.999 | Prec: 0.999 | Rec: 0.999 | F1: 0.999\nEpoch 10 | Train Loss: 0.004 | Acc: 0.999 | Prec: 0.999 | Rec: 0.999 | F1: 0.999\n\nTest Results | Loss: 4.119 | Acc: 0.462 | Prec: 0.423 | Rec: 0.199 | F1: 0.271\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(\"\\nExperiment 3: Train and Test on Articles\")\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(articles_texts, articles_labels, test_size=0.2, random_state=42)\nrun_experiment(train_texts, train_labels, test_texts, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T11:54:43.699184Z","iopub.execute_input":"2024-12-01T11:54:43.699608Z","iopub.status.idle":"2024-12-01T12:04:36.837211Z","shell.execute_reply.started":"2024-12-01T11:54:43.699571Z","shell.execute_reply":"2024-12-01T12:04:36.835640Z"}},"outputs":[{"name":"stdout","text":"\nExperiment 3: Train and Test on Articles\nEpoch 1 | Train Loss: 0.406 | Acc: 0.801 | Prec: 0.832 | Rec: 0.880 | F1: 0.855\nEpoch 2 | Train Loss: 0.210 | Acc: 0.913 | Prec: 0.931 | Rec: 0.940 | F1: 0.935\nEpoch 3 | Train Loss: 0.122 | Acc: 0.952 | Prec: 0.965 | Rec: 0.963 | F1: 0.964\nEpoch 4 | Train Loss: 0.066 | Acc: 0.975 | Prec: 0.982 | Rec: 0.981 | F1: 0.982\nEpoch 5 | Train Loss: 0.041 | Acc: 0.985 | Prec: 0.989 | Rec: 0.989 | F1: 0.989\nEpoch 6 | Train Loss: 0.022 | Acc: 0.994 | Prec: 0.995 | Rec: 0.995 | F1: 0.995\nEpoch 7 | Train Loss: 0.015 | Acc: 0.996 | Prec: 0.997 | Rec: 0.997 | F1: 0.997\nEpoch 8 | Train Loss: 0.012 | Acc: 0.997 | Prec: 0.997 | Rec: 0.998 | F1: 0.998\nEpoch 9 | Train Loss: 0.008 | Acc: 0.998 | Prec: 0.998 | Rec: 0.998 | F1: 0.998\nEpoch 10 | Train Loss: 0.005 | Acc: 0.999 | Prec: 0.999 | Rec: 0.999 | F1: 0.999\n\nTest Results | Loss: 0.044 | Acc: 0.986 | Prec: 0.996 | Rec: 0.983 | F1: 0.989\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(\"\\nExperiment 4: Train and Test on Tweets\")\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(tweets_texts, tweets_labels, test_size=0.2, random_state=42)\nrun_experiment(train_texts, train_labels, test_texts, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T12:04:36.839765Z","iopub.execute_input":"2024-12-01T12:04:36.840186Z","iopub.status.idle":"2024-12-01T12:06:05.671302Z","shell.execute_reply.started":"2024-12-01T12:04:36.840124Z","shell.execute_reply":"2024-12-01T12:06:05.670133Z"}},"outputs":[{"name":"stdout","text":"\nExperiment 4: Train and Test on Tweets\nEpoch 1 | Train Loss: 0.646 | Acc: 0.604 | Prec: 0.603 | Rec: 0.623 | F1: 0.613\nEpoch 2 | Train Loss: 0.430 | Acc: 0.820 | Prec: 0.813 | Rec: 0.836 | F1: 0.824\nEpoch 3 | Train Loss: 0.307 | Acc: 0.898 | Prec: 0.893 | Rec: 0.905 | F1: 0.899\nEpoch 4 | Train Loss: 0.220 | Acc: 0.935 | Prec: 0.935 | Rec: 0.935 | F1: 0.935\nEpoch 5 | Train Loss: 0.164 | Acc: 0.954 | Prec: 0.956 | Rec: 0.952 | F1: 0.954\nEpoch 6 | Train Loss: 0.121 | Acc: 0.974 | Prec: 0.974 | Rec: 0.973 | F1: 0.974\nEpoch 7 | Train Loss: 0.080 | Acc: 0.983 | Prec: 0.981 | Rec: 0.986 | F1: 0.983\nEpoch 8 | Train Loss: 0.065 | Acc: 0.987 | Prec: 0.987 | Rec: 0.988 | F1: 0.987\nEpoch 9 | Train Loss: 0.047 | Acc: 0.994 | Prec: 0.996 | Rec: 0.992 | F1: 0.994\nEpoch 10 | Train Loss: 0.036 | Acc: 0.995 | Prec: 0.996 | Rec: 0.995 | F1: 0.995\n\nTest Results | Loss: 0.723 | Acc: 0.787 | Prec: 0.959 | Rec: 0.594 | F1: 0.733\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(\"\\nExperiment 5: Train and Test on Combined\")\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(combined_texts, combined_labels, test_size=0.2, random_state=42)\nrun_experiment(train_texts, train_labels, test_texts, test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T12:06:05.672614Z","iopub.execute_input":"2024-12-01T12:06:05.673030Z","iopub.status.idle":"2024-12-01T12:17:18.301495Z","shell.execute_reply.started":"2024-12-01T12:06:05.672996Z","shell.execute_reply":"2024-12-01T12:17:18.300009Z"}},"outputs":[{"name":"stdout","text":"\nExperiment 5: Train and Test on Combined\nEpoch 1 | Train Loss: 0.462 | Acc: 0.772 | Prec: 0.801 | Rec: 0.858 | F1: 0.829\nEpoch 2 | Train Loss: 0.257 | Acc: 0.891 | Prec: 0.914 | Rec: 0.917 | F1: 0.915\nEpoch 3 | Train Loss: 0.150 | Acc: 0.941 | Prec: 0.954 | Rec: 0.954 | F1: 0.954\nEpoch 4 | Train Loss: 0.084 | Acc: 0.971 | Prec: 0.976 | Rec: 0.978 | F1: 0.977\nEpoch 5 | Train Loss: 0.051 | Acc: 0.983 | Prec: 0.987 | Rec: 0.986 | F1: 0.987\nEpoch 6 | Train Loss: 0.032 | Acc: 0.989 | Prec: 0.992 | Rec: 0.992 | F1: 0.992\nEpoch 7 | Train Loss: 0.020 | Acc: 0.994 | Prec: 0.994 | Rec: 0.996 | F1: 0.995\nEpoch 8 | Train Loss: 0.015 | Acc: 0.995 | Prec: 0.996 | Rec: 0.996 | F1: 0.996\nEpoch 9 | Train Loss: 0.012 | Acc: 0.996 | Prec: 0.997 | Rec: 0.997 | F1: 0.997\nEpoch 10 | Train Loss: 0.008 | Acc: 0.997 | Prec: 0.997 | Rec: 0.998 | F1: 0.998\n\nTest Results | Loss: 0.135 | Acc: 0.965 | Prec: 0.971 | Rec: 0.973 | F1: 0.972\n","output_type":"stream"}],"execution_count":30}]}