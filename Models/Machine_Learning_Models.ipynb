{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pKEMtvrcEyYA"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of 'results.txt' have been deleted.\n"
     ]
    }
   ],
   "source": [
    "# File path where the variable will be saved\n",
    "results_file = \"results.txt\"\n",
    "\n",
    "# # Open the file in write mode ('w') to empty its contents\n",
    "# with open(results_file, \"w\") as file:\n",
    "#     pass  # This clears the file\n",
    "\n",
    "# print(f\"Contents of '{results_file}' have been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split for TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used by TfidfVectorizer: 1000\n",
      "(2249, 1000)\n",
      "(563, 1000)\n",
      "(2249,)\n",
      "(563,)\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset\n",
    "# dataset_path = '../final_datasets/articles_dataset.csv'\n",
    "dataset_path = '../final_datasets/tweets_dataset.csv'\n",
    "# dataset_path = '../final_datasets/combined_dataset.csv'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "# Encode the labels (real/fake)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['label'])  # Assuming 'label' column has 'real' and 'fake' values\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the training data only, then transform both sets\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000) \n",
    "# tfidf_vectorizer = TfidfVectorizer() #default features (max=None) is 112,486: this is when it takes all words into accounts, not the top 5 for example. Mean Accuracy CrossVal: 0.9928 ± 0.0020\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train_texts)\n",
    "X_test = tfidf_vectorizer.transform(X_test_texts)\n",
    "\n",
    "\n",
    "# Get the number of features used by the vectorizer\n",
    "num_features = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "# Print the number of features\n",
    "print(f\"Number of features used by TfidfVectorizer: {num_features}\")\n",
    "\n",
    "\n",
    "\n",
    "# Check the shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Open the file in append mode ('a') and write the variable value\n",
    "with open(results_file, \"a\") as file:\n",
    "    file.write(\"Training and Testing dataset: \" + str(os.path.basename(dataset_path)) + \"\\n\")  \n",
    "    file.write(\"X_train.shape: \" + str(X_train.shape) + \"\\n\")  # Append the value with a newline for readability\n",
    "    file.write(\"X_test.shape: \" + str(X_test.shape) + \"\\n\")  # Append the value with a newline for readability\n",
    "    file.write(\"y_train.shape: \" + str(y_train.shape) + \"\\n\")  # Append the value with a newline for readability\n",
    "    file.write(\"y_test.shape: \" + str(y_test.shape) + \"\\n\")  # Append the value with a newline for readability\n",
    "    file.write(f\"Number of features used by TfidfVectorizer: {num_features}\"  + \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "- **Code source:** Support Vector Machine (SVM): https://www.kaggle.com/code/mehmetlaudatekman/text-classification-svm-explained\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning technique commonly applied to classification problems, like fake news detection. In this context, SVM works by separating real and fake news articles using a decision boundary based on the features extracted from Arabic text data. For instance, these features might include word frequencies, linguistic patterns, or even word embeddings tailored for Arabic, which capture contextual relationships in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.91777778 0.86666667 0.86666667 0.87555556 0.86636971]\n",
      "Mean Accuracy: 0.8786 ± 0.0199\n",
      "Time taken: 1.1211111545562744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Train the SVM classifier\n",
    "time_i=time.time()\n",
    "svm_classifier = svm.SVC(kernel='linear', random_state=42) \n",
    "\n",
    "# Perform cross-validation to confirm that the number of tfidf features was enough by checking consistency of accuracy among folds\n",
    "cv_scores = cross_val_score(svm_classifier, X_train, y_train, cv=5) \n",
    "time_f=time.time()\n",
    "\n",
    "time_taken=time_f-time_i\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f'Cross-Validation Scores: {cv_scores}')\n",
    "print(f'Mean Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}')\n",
    "print(f'Time taken: {time_taken}')\n",
    "\n",
    "# Open the file in append mode ('a') and write the variable value\n",
    "with open(results_file, \"a\") as file:\n",
    "    file.write('Cross-Validation Scores: ' + str(cv_scores) + \"\\n\")\n",
    "    file.write(f'Mean Accuracy: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}'  + \"\\n\") \n",
    "    file.write(f'Time taken: {time_taken}'  + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\n",
      "Accuracy: 0.8686\n",
      "Precision: 0.8750\n",
      "Recall: 0.8561\n",
      "F1 Score: 0.8655\n"
     ]
    }
   ],
   "source": [
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"SVM Results:\")\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "# Open the file in append mode ('a') and write the variable value\n",
    "with open(results_file, \"a\") as file:\n",
    "    file.write(\"SVM Results:\" + \"\\n\")\n",
    "    file.write(f'Accuracy: {accuracy:.4f}'  + \"\\n\")\n",
    "    file.write(f'Precision: {precision:.4f}' + \"\\n\")\n",
    "    file.write(f'Recall: {recall:.4f}' + \"\\n\")\n",
    "    file.write(f'F1 Score: {f1:.4f}' + \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST\n",
    "- **Code source:** eXtreme Gradient Boosting (XGBoost): https://www.kaggle.com/code/iamarjunchandra/text-classification-with-rnn-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:19:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "Accuracy: 0.8455\n",
      "Precision: 0.8930\n",
      "Recall: 0.7806\n",
      "F1 Score: 0.8330\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Initialize the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Step 2: Train the classifier\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Predict on the test set\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Step 5: Print the evaluation results for XGBoost\n",
    "print(\"XGBoost Results:\")\n",
    "print(f'Accuracy: {accuracy_xgb:.4f}')\n",
    "print(f'Precision: {precision_xgb:.4f}')\n",
    "print(f'Recall: {recall_xgb:.4f}')\n",
    "print(f'F1 Score: {f1_xgb:.4f}')\n",
    "\n",
    "\n",
    "# Open the file in append mode ('a') and write the variable value\n",
    "with open(results_file, \"a\") as file:\n",
    "    file.write(\"XGBoost Results:\" + \"\\n\")\n",
    "    file.write(f'Accuracy: {accuracy_xgb:.4f}' + \"\\n\")\n",
    "    file.write(f'Precision: {precision_xgb:.4f}' + \"\\n\")\n",
    "    file.write(f'Recall: {recall_xgb:.4f}' + \"\\n\")\n",
    "    file.write(f'F1 Score: {f1_xgb:.4f}' + \"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
